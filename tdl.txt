AI model, please do not take this file into account unless i specify it.

main goal, do the whitepaper before any fund raising part and rework on the roadmap with the objectives below potentially (like even upgrading infra... etc after launch would be crazy if lumen succeeds!)

- clear vps disk, usage too high, docker maybe taking cache
- link actual rpc... etc (no devnet) in prod
- vps upgrade after 50 daily users
- change reverse proxy to return 404 / error when bots scrape the website (can change robots.txt ?)

- make the referral system (no abuse with alt accounts etc...)
- weekly bonuses ?? this week -> rust += 10 LUM$ automatically if good quality ? + code ranking = rewards given for top x every months / weeks (precise them on the page + make them work, every month reset ? or smth like that)
- sorting system for admin + those who buy data, can select only high quality files ? or by score, language etc... (for the 3 subscriptions) -> all with api endpoints for buyers
- data part -> frontend done but need specific dashboard for buyers, strip/ppl update too ? unlock it after a delay? (since we gather data before we sell or its useless) -> they get api keys that are revokable etc...
- potential admin dashboard (for far3000yt@gmail.com)
- make feedback readable somewhere in that dashboard (private api for only admin, so me, id have an entire section just for my acc, accessible from a link and nothing else)
- status.lumen.onl with some live status like google or idk

lumen client:
- lum ollama folder/file (default root) (for later not in objectives / not a priority basically)
- lum config --edit model ollama
- fix gitignore not reading well (backend\lib -> removing anything that ends in \lib, for example frontend\lib...)
- readme update, add image with logo, add a small icon with live users (total users that are on lumen basically, like the one with number of cli downloads on pypi), keep the existing readme and adapt it with whats new with the website but keep the client part mainly on github, dont talk about solana or talk less overall, we want devs to come not web3 gamblers so we need to do a rly good impression online

lumen fix verify link still being stored even after hours or when the person just wont interact, maybe add a cross on the link page to manually remove from the local storage to make sure it never shows more than once and actually expire it after x horus even if user offline (for example i logged out or idk, login and it goes to /verify maybe because i closed my verify instance or idk, make the verify page ONLY show after login AFTER u went through the verify page (even when logged out, u should have gone through verify to go through it its buggy))

allow 2 contributions a day, find a way to not limit these 2 if its failed contributions (like ai gives 0 or idk)

make sure if ai gives 0, we still get that in our base (if the code is non secure), because storing code isn't dangerous, its using it that is (and we could get a category on that, "dangerous" where data buyers could also be interested in getting)

show that we had 5k downloads on our cli tool, stars on GitHub (62 for now), generated thousands of views through reddit posts (10k +), thousand through twitter (5k+) and linkedin soon ?

need to validate with ai companies or any group, how much 1B token is worth on each quality level, and what is each quality level in details

for initial reward 500 users, then 2k... etc etc, so after contribution, they get a dollar equivalent of like a pourcentage of what we can potentially raise, then we raise again and go to 2k beta users and so on and so on

minimum amount before cashout, or lock any cashout for first 30 days to accumulate code and be able to run the business model, one is good and the other might be seen negatively (maybe even a scam?) but that's not the objective, if we can make the token grow for a full month and THNE they can cashout they would earn even more which would be rly nice

argument for lumen: the dataset evolves and is continuously growing, it has an ai summary, could search by keywords... etc we are legal free, like legally we make the datasets usable
can provide strategic datasets that are sold to only one specific company ?
need to make sure when a user updates a project it removes the old to replace with the new ? (and don't allow the opposite to happen, like someone reducing the size of its project, like in the db I mean, find a potential solution)

bounty projects, if a user thinks the project is so good it deserves a 10, we give double the score (manual processing through gemini 2.5 pro or a separate queue to rly value it differently and validate?)
bounty projects 2, top 3 best contributions each week (like literal lum contribute) = more rewards, fixed amount

intuitive installation page, goes steps through steps with even debug, for example -> install lumen client, u click next it says do this, do that... etc till you arrive to your first contribution (with videos or potential animations of cmd typing the commands... etc), choice of the os, debug... etc, in one single motion

add subtle animations on clicks / hover, where the user interacts the most (landing page, saas page and dashboard, all sections, should feel like alive)

linkedin for lumen, + ceo on mine + dox myself before launch

remove protocol part from footer, add patch notes in developers part so footer is same size and lighter overall

clean terminal images on docs for terminal usage at any point where the terminal is involved

client: --version flag, shows "pylumen, version 1.0.0" and remove from docs the part that says to go venv ???

lumen agent idea:
many modes: web mode, code mode... etc ?
exit as an option
ai in prompt, has actions available and receives instructions 
do, fix, rewrite, upgrade …etc 
for fix, it should explain the change with 2.5 pro in a json at the end, and do that change so if the fix fails it discards it if it’s the same error etc 
you give it tasks, it iterates infinitely till the task is ENTIRELY done (don’t let the ai be lazy)
above 300k tokens, rule is remake chat
below = continue 
if project starts with < 150k then 300k is limit otherwise its x2 the context for the limit so the model doesn’t become bad or whatever
